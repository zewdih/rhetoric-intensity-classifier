{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624c445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev set: 0.67\n",
      "Accuracy on test set: 0.65\n",
      "95% Confidence Interval for test set accuracy: (0.55, 0.74)\n"
     ]
    }
   ],
   "source": [
    "#Necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Reading datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "dev = pd.read_csv(\"dev.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove rows where Label == \"Apathetic\" from train, dev, and test\n",
    "train = train[train['Label'] != 'Apathetic']\n",
    "dev = dev[dev['Label'] != 'Apathetic']\n",
    "test = test[test['Label'] != 'Apathetic']\n",
    "\n",
    "#Converts text labels into numbers (so the model can understand them)\n",
    "label_encoder = LabelEncoder()\n",
    "train['Encoded Label'] = label_encoder.fit_transform(train['Label'])\n",
    "\n",
    "#Building model\n",
    "# Step 1: Turn the text into numbers (TF-IDF) so model capture their meaning & importance\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter= 400, C = 1.2, penalty='l2')\n",
    ")\n",
    "\n",
    "#Step 2: Train the Multi-Logistic Regression model on those numbers\n",
    "pipeline.fit(train['Original Text'], train['Encoded Label'])\n",
    "\n",
    "#Define the parameter combinations of C and max_iter to try\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.1, 0.5, 1.0, 1.2, 2.0],\n",
    "    'logisticregression__max_iter': [100, 200, 400, 500]\n",
    "}\n",
    "\n",
    "#Use GridSearchCV with 5-fold cross-validation to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(dev['Original Text'], label_encoder.transform(dev['Label']))\n",
    "\n",
    "# Update model with the best parameters\n",
    "pipeline.set_params(**grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the dev set\n",
    "dev_predictions = pipeline.predict(dev['Original Text'])\n",
    "# Convert the predicted numbers back into original label names\n",
    "dev['Predicted Label'] = label_encoder.inverse_transform(dev_predictions)\n",
    "\n",
    "# Calculate accuracy on the dev set\n",
    "accuracy = (dev['Label'] == dev['Predicted Label']).mean()\n",
    "print(f\"Accuracy on dev set: {accuracy:.2f}\")\n",
    "\n",
    "#Merge train and dev to retrain final model so it can learn from all available data\n",
    "full_train = pd.concat([train, dev])\n",
    "\n",
    "# Retrain model on full data\n",
    "pipeline.fit(full_train['Original Text'], label_encoder.transform(full_train['Label']))\n",
    "\n",
    "# Training model on test set\n",
    "test_predictions = pipeline.predict(test['Original Text'])\n",
    "test['Predicted Label'] = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_test = (test['Label'] == test['Predicted Label']).mean()\n",
    "print(f\"Accuracy on test set: {accuracy_test:.2f}\")\n",
    "\n",
    "# Calculate the standard error\n",
    "n_test = len(test)\n",
    "std_error = (accuracy_test * (1 - accuracy_test) / n_test) ** 0.5\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "z_score = norm.ppf(0.975)  # 95% confidence\n",
    "ci_lower = accuracy_test - z_score * std_error\n",
    "ci_upper = accuracy_test + z_score * std_error\n",
    "\n",
    "\n",
    "print(f\"95% Confidence Interval for test set accuracy: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
    "#print(dev['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a613bd",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "Our team created a baseline majority classifier, which always predicts the most frequent label. It achieved 44% accuracy on the dev set and 38% on the test set. This provides a helpful baseline for comparing subsequent models to ensure that the models we're building are doing better than chance.\n",
    "\n",
    "Upon building a trained TF-IDF + Logistic Regression model, it achieved the following accuracies (after fine-tuning hyperparameters via cross-validation on the development set):\n",
    "\n",
    "67% accuracy on the dev set\n",
    "65% accuracy on the test set\n",
    "A 95% confidence interval of (55%, 74%) on the test set\n",
    "\n",
    "Before training the final model, we identified that one class (\"Apathetic\") had only a single data point. To avoid unstable cross-validation and unreliable model behavior, we removed this class from the dataset. This allowed for more robust training and improved the overall model performance.\n",
    "\n",
    "To fine-tune hyperparameters, we performed 5-fold cross-validation on the dev set, testing multiple values for C and max_iter. This allowed for more reliable parameter selection and helped us avoid overfitting to any particular fold of the data.\n",
    "\n",
    "These results reflect a substantial improvement of +23% on the dev set and +27% on the test set compared to the majority baseline. Notably, even the lower bound of the confidence interval exceeds the majority classifier’s test accuracy, indicating that the model generalizes better to unseen data and is learning patterns within the data it’s trained upon.\n",
    "\n",
    "We can conclude from this that the model’s hyperparameter tuning — in particular, setting C = 1.2 and decreasing the number of iterations to 400 (since our train set has fewer than 10k data points) — improves model performance. Cross-validation was essential in selecting these values, as larger values than 1.2 for C decreased model accuracy, and changing the number of iterations had no meaningful effect.\n",
    "\n",
    "The differences in performance between the baseline model and the one I trained illustrate that the model isn’t just guessing the most common answer — it’s actually learning from the text and making smarter, more accurate predictions based on what it reads.\n",
    "\n",
    "We note that there is a small decrease in accuracy from the dev set (67%) to the test set (65%). Since model hyperparameters were tuned on the development set, it is expected that performance on the unseen test set may be slightly lower. This small drop suggests that the model has generalized well and is not overfitting to the development data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arm-mpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
